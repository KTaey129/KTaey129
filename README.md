# 👋 Hey there, I'm Taeyang Koh

I'm passionate about **cloud-based data engineering**—from spinning up DynamoDB tables and RDS instances, to orchestrating real-time streaming pipelines on AWS. 
Through hands-on projects like "Database Connectivity on AWS” and building entire data lakehouses with **Lake Formation + Apache Iceberg**, I've learned how to design, optimize, and troubleshoot data workflows in real-world scenarios.

---

### 🚀 What I'm Working On

- **Database Connectivity on AWS**  
  Developing Python + JDBC prototypes to improve data ingestion, queries, and performance. Learned to solve connection errors, indexing strategies, and query optimizations.

- **Real-Time Streaming with Kinesis**  
  Experimenting with Kinesis Data Streams and AWS Lambda to collect & process data on the fly.
  Focused on reducing stream bottlenecks via logging, CloudWatch metrics, and DynamoDB integration.

- **Data Lakehouse & Orchestration**  
  Prototyping a medallion architecture (landing → curated → presentation) using AWS Lake Formation, Iceberg tables, and Terraform for reproducible infrastructure.
  Running queries in Athena and managing code in AWS Glue.

- **Spark & Flink for Large-Scale ETL**  
  Building distributed workflows on AWS EMR (PySpark) and Apache Flink to handle high-volume data sets.
  Emphasizing real-time analytics, robust CDC pipelines, and data transformations at scale.

---

### 🧰 Tech & Tools

- **AWS**: RDS, DynamoDB, Kinesis, CloudWatch, S3, Glue, Athena  
- **Data Workflows**: Python (data pipelines), SQL (advanced queries), dbt (modeling), Terraform (IaC)  
- **Streaming / Processing**: Apache Flink, Spark, Debezium, EMR  
- **Data Storage**: Lake Formation, Iceberg, Redshift Spectrum  
- **Analytics & Visualization**: Jupyter/Zeppelin Notebooks, Airflow orchestration, Superset dashboards  

---

### 🎯 Goals

- **Refine Cloud-Native Data Pipelines**  
  Keep evolving streaming solutions (like Kinesis → Flink → Lakehouse) for real-time analytics.  
- **Deepen Data Modeling Expertise**  
  Continue exploring advanced SQL patterns, normalization strategies, and dbt for robust warehousing.  
- **Expand Big Data Tooling**  
  Incorporate more Spark-based transformations and explore new ways of automating large ETL processes.  
- **Explore AI/ML Integration**  
  Bridge data engineering to ML pipelines, focusing on text data embeddings and streaming predictions.

---

### 📚 Currently Learning

- **Performance Tuning** in row vs. column-oriented DBs (Amazon Redshift vs. PostgreSQL).  
- **Advanced Orchestration** with Airflow for data quality checks, dbt runs, and DAG-based pipeline design.  
- **NLP & Vector Databases** to handle text data embeddings and vector search (pgvector, Faiss, etc.).

---

### 📫 Let's Connect

- **LinkedIn**: https://www.linkedin.com/in/taeyang-koh/
- **Email**: ktaey129@gmail.com

---

> “Never stop iterating on data solutions—because every bottleneck is just an invitation to learn and optimize.”
